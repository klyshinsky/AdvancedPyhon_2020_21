{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наследование в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Облагораживаем код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть два фрагмента кода, которые выкачивают Ленту.ру и N+1. Как это выглядит показано ниже. Давайте попытаемся сделать что-то получше с учетом того, что на свете существует наследование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Регулярные выражения.\n",
    "import requests # Загрузка новостей с сайта.\n",
    "from bs4 import BeautifulSoup # Превращалка html в текст.\n",
    "import pymorphy2 # Морфологический анализатор.\n",
    "import datetime # Новости будем перебирать по дате.\n",
    "from collections import Counter # Не считать же частоты самим.\n",
    "import math # Корень квадратный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это класс для загрузки Ленты.ру. Он позволяет загрузить статьи с сайта, сохранить их в файл считать этот файл, создать частотный словарь новостей.\n",
    "\n",
    "Класс хранит все новости, заголовки и словари в отдельных списках.\n",
    "\n",
    "Сайт удобен тем, что по определенному адресу (https://lenta.ru/news/год/месяц/день/) доступны ссылки на все статьи за данный день. Они хранят архив с 1999 года."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getNewsPaper:\n",
    "        \n",
    "    # Конструктор - вызывается при создании объекта и инициализирует его.\n",
    "    def __init__(self, filename=None):\n",
    "        self.articles=[]     # Загруженные статьи.\n",
    "        self.titles=[]       # Заголовки статей.\n",
    "        self.dictionaries=[] # Словари для каждой из статей.\n",
    "        # Создаем и загружаем морфологический словарь.\n",
    "        self.morph=pymorphy2.MorphAnalyzer()\n",
    "        if filename!=None:\n",
    "            self.loadArticles(filename)\n",
    "\n",
    "    # Загрузка статьи по URL.\n",
    "    def getLentaArticle(self, url):\n",
    "        \"\"\" getLentaArticle gets the body of an article from Lenta.ru\"\"\"\n",
    "        # Получает текст страницы.\n",
    "        resp=requests.get(url)\n",
    "        # Загружаем текст в объект типа BeautifulSoup.\n",
    "        bs=BeautifulSoup(resp.text, \"html5lib\") \n",
    "        # Получаем заголовок статьи.\n",
    "        self.titles.append(bs.h1.text.replace(\"\\xa0\", \" \"))\n",
    "        # Получаем текст статьи.\n",
    "        self.articles.append(BeautifulSoup(\" \".join(\n",
    "                    [p.text for p in bs.find_all(\"p\")]), \"html5lib\").get_text().replace(\"\\xa0\", \" \"))\n",
    "\n",
    "    # Загрузка всех статей за один день.\n",
    "    def getLentaDay(self, url):\n",
    "        \"\"\" Gets all URLs for a given day and gets all texts. \"\"\"\n",
    "        try:\n",
    "            # Грузим страницу со списком всех статей.\n",
    "            day = requests.get(url) \n",
    "            # Получаем фрагменты с нужными нам адресами статей.\n",
    "            h3s=BeautifulSoup(day.text, \"html5lib\").find_all(\"h3\")\n",
    "            # Получаем все адреса на статьи за день.\n",
    "            links=[\"http://lenta.ru\"+l.find_all(\"a\")[0][\"href\"] for l in h3s]\n",
    "            # Загружаем статьи.\n",
    "            for l in links:\n",
    "                self.getLentaArticle(l)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getLentaPeriod(self, start, finish):\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getLentaDay('https://lenta.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "    # Построение вектора для статьи.\n",
    "    posConv={'ADJF':'_ADJ','NOUN':'_NOUN','VERB':'_VERB'}\n",
    "    def getArticleDictionary(self, text, needPos=None):\n",
    "        words=[a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        reswords=[]\n",
    "    \n",
    "        for w in words:\n",
    "            wordform=self.morph.parse(w)[0]\n",
    "            try:\n",
    "                if wordform.tag.POS in ['ADJF', 'NOUN', 'VERB']:\n",
    "                    if needPos!=None:\n",
    "                        reswords.append(wordform.normal_form+self.posConv[wordform.tag.POS])\n",
    "                    else:\n",
    "                        reswords.append(wordform.normal_form)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        stat=Counter(reswords)\n",
    "        # Берем только слова с частотой больше 1.\n",
    "        stat={a: stat[a] for a in stat.keys() if stat[a]>1}\n",
    "        return stat\n",
    "\n",
    "    # Посчитаем вектора для всех статей.\n",
    "    def calcArticleDictionaries(self, needPos=None):\n",
    "        self.dictionaries=[]\n",
    "        for a in self.articles:\n",
    "            self.dictionaries.append(self.getArticleDictionary(a, needPos))\n",
    "            \n",
    "    # Сохраняем статьи в файл.\n",
    "    def saveArticles(self, filename):\n",
    "        \"\"\" Saves all articles to a file with a filename. \"\"\"\n",
    "        newsfile=open(filename, \"w\")\n",
    "        for art, titl in zip(self.articles, self.titles):\n",
    "            newsfile.write('\\n=====\\n'+titl)\n",
    "            newsfile.write('\\n-----\\n'+art)\n",
    "        newsfile.close()\n",
    "\n",
    "    # Читаем статьи из файла.\n",
    "    def loadArticles(self, filename):\n",
    "        \"\"\" Loads and replaces all articles from a file with a filename. \"\"\"\n",
    "        newsfile=open(filename, encoding=\"utf-8\")\n",
    "        text=newsfile.read()\n",
    "        self.articles=text.split('\\n=====\\n')[1:]\n",
    "        for i, a in enumerate(self.articles):\n",
    "            b, self.articles[i] = a.split('\\n-----\\n')\n",
    "            self.titles.append(b)\n",
    "        newsfile.close()\n",
    "\n",
    "    # Для удобства - поиск статьи по ее заголовку.\n",
    "    def findNewsByTitle(self, title):\n",
    "        if title in self.titles:\n",
    "            return self.titles.index(title)\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это класс для загрузки сайта N+1.ru. Его структура очень сильно схожа со структурой Ленты.ру, именно поэтому мы его и берем. \n",
    "\n",
    "Здесь заведен тип под новостную заметку, так как для нее хранится гораздо больше информации: время, дата, рубрика, сложность, автор, заголовок, текст. Помимо этого, статья умеет сохранять себя в [JSON](https://ruseller.com/lessons.php?id=1212) (без использования соответствующей [библиотеки](https://pythonworld.ru/moduli/modul-json.html)) и словарь Питона.\n",
    "\n",
    "Дальше идут две функции, которые выгружают отдельную статью и все статьи за день."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delcom=re.compile(\"<!--.+-->\", re.S)\n",
    "\n",
    "# Класс, хранящий информацию о статье.\n",
    "class NPlus1Article:\n",
    "    def __init__(self):\n",
    "        self.time=\"\"\n",
    "        self.date=\"\"\n",
    "        self.rubr=\"\"\n",
    "        self.diff=\"\"\n",
    "        self.author=\"\"\n",
    "        self.title=\"\"\n",
    "        self.text=\"\"\n",
    "        \n",
    "    # Конвертация в JSON.\n",
    "    def toJSON(self):\n",
    "        res='{\"date\":\"'+self.date+'\", \"time\":\"'+self.time+'\", \"rubrics\":\"'+self.rubr+'\", \"difficulty\":\"'\n",
    "        res+=self.diff+'\", \"title\":\"'+self.head+'\", \"author\":\"'+self.author+'\",\"text\":\"'\n",
    "        res+=self.text.replace('\"', '\\\\\"')+'\"}'\n",
    "        return res\n",
    "\n",
    "    # Конвертация в словарь.\n",
    "    def toDict(self):\n",
    "        res={\"date\":self.date, \"time\":self.time, \"rubrics\":self.rubr, \"difficulty\":self.diff,\\\n",
    "             \"title\":self.head, \"author\":self.author,\"text\":self.text.replace('\"', '\\\\\"')}\n",
    "        return res\n",
    "    \n",
    "def getArticleTextNPlus1(adr):\n",
    "    r = requests.get(adr)\n",
    "    #print(r.text)\n",
    "    art = NPlus1Article()\n",
    "    tables = re.split(\"</div>\",re.split('=\"tables\"', r.text)[1])[0]\n",
    "    t1 = re.split(\"</time>\", re.split(\"<time\", tables)[1])[0]\n",
    "    art.time = re.split(\"</span>\", re.split(\"<span>\", t1)[1])[0]\n",
    "    art.date = re.split(\"</span>\", re.split(\"<span>\", t1)[2])[0]\n",
    "    art.rubr = re.findall(\"<a data-rubric.+?>(.+?)</a>\", r.text)[0]\n",
    "    art.diff = re.split(\"</span>\", re.split('\"difficult-value\">', tables)[1])[0]\n",
    "    art.head = re.split(\"</h1>\",re.split('<h1>', r.text)[1])[0]\n",
    "    art.author = re.split('\" />',re.split('<meta name=\"author\" content=\"', r.text)[1])[0]\n",
    "    art.text = re.split(\"</div>\", re.split(\"</figure>\", re.split('</article>',re.split('<article', r.text)[1])[0])[1])[1]    \n",
    "\n",
    "    beaux_text = BeautifulSoup(art.text, \"html5lib\")\n",
    "    art.text = delcom.sub(\"\", beaux_text.get_text() )\n",
    "    art.text = art.text.replace('\\xa0', ' ')\n",
    "\n",
    "    # print(art.n_time, art.n_date, art.n_rubr, art.n_diff)\n",
    "    # print(art.n_head)\n",
    "    # print(art.n_author)\n",
    "    # print(art.n_text)\n",
    "    #return [n_time, n_date, n_rubr, n_diff, n_author, n_head, n_text]\n",
    "    return art\n",
    "\n",
    "def getDayArticles(adr):\n",
    "    r = requests.get(adr)\n",
    "    titles = BeautifulSoup(r.text, \"html5lib\")(\"article\")\n",
    "    #print(titles)\n",
    "    addrs = [\"https://nplus1.ru/\"+a(\"a\")[0][\"href\"] for a in titles]\n",
    "    #print(addrs)\n",
    "    articles = []\n",
    "    for adr in addrs:\n",
    "        articles.append(getArticleTextNPlus1(adr))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте для начала откажемся от классов, которые выкачивают данные, и сосредоточимся на самих данных, которые они возвращают. Давайте считать, что заголовок и текст есть у любой статьи, а всё остальное - это дополнительные детали, зависящие от конкретного сайта.\n",
    "\n",
    "Создадим класс базовой статьи, который будет содержать в себе только два поля. [Унаследуем](https://o7planning.org/ru/11417/inheritance-and-polymorphism-in-python) от него класс статьи для N+1 - NPlus1Article. Теперь можно считать, что любая функция возвращает объект, который ведет себя как объект BaseArticle - у него есть поля title и text.\n",
    "\n",
    "В конструкторе NPlus1Article вызывается конструктор базового класса. При помощи функции `super()` мы обращаемся к объекты как к объекту родительского класса (нам даже не важно знать какого).\n",
    "\n",
    "----\n",
    "\n",
    "Вообще, наследование необходимо для трех вещей:\n",
    "- расширить функционал или набор данных имеющегося класса;\n",
    "- взять несколько классов с общей частью и выделить ее в единый базовый класс **с выделением соответствующей сущности**.\n",
    "- обеспечить единый интерфейс для классов-наследников (это скорее подход для [языков группы Вирта](https://habr.com/ru/post/303380/), в Питоне часто используется интерфейс по договоренности, но наследование помогает упросить понимание)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseArticle:\n",
    "    def __init__(self):\n",
    "        self.title=\"\"\n",
    "        self.text=\"\"\n",
    "        \n",
    "class NPlus1Article(BaseArticle):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time=\"\"\n",
    "        self.date=\"\"\n",
    "        self.rubr=\"\"\n",
    "        self.diff=\"\"\n",
    "        self.author=\"\"\n",
    "        \n",
    "    def getSuper(self):\n",
    "        return super(NPlus1Article, self)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<super: __main__.NPlus1Article, <__main__.NPlus1Article at 0x7f940407df60>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr=NPlus1Article()\n",
    "rr.getSuper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оформим загрузку статей как функции и напишем еще одну, которая выгружает статью в JSON, список выгружаемых полей зависит от типа переменной. Проверка типа проводится при помощи функции `isinstance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticleTextNPlus1(adr):\n",
    "    r = requests.get(adr)\n",
    "    #print(r.text)\n",
    "    art = NPlus1Article()\n",
    "    tables = re.split(\"</div>\",re.split('=\"tables\"', r.text)[1])[0]\n",
    "    t1 = re.split(\"</time>\", re.split(\"<time\", tables)[1])[0]\n",
    "    art.time = re.split(\"</span>\", re.split(\"<span>\", t1)[1])[0]\n",
    "    art.date = re.split(\"</span>\", re.split(\"<span>\", t1)[2])[0]\n",
    "    art.rubr = re.findall(\"<a data-rubric.+?>(.+?)</a>\", r.text)[0]\n",
    "    art.diff = re.split(\"</span>\", re.split('\"difficult-value\">', tables)[1])[0]\n",
    "    art.head = re.split(\"</h1>\",re.split('<h1>', r.text)[1])[0]\n",
    "    art.author = re.split('\" />',re.split('<meta name=\"author\" content=\"', r.text)[1])[0]\n",
    "    art.text = re.split(\"</div>\", re.split(\"</figure>\", re.split('</article>',re.split('<article', r.text)[1])[0])[1])[1]    \n",
    "\n",
    "    beaux_text = BeautifulSoup(art.text, \"html5lib\")\n",
    "    art.text = delcom.sub(\"\", beaux_text.get_text() )\n",
    "    art.text = art.text.replace('\\xa0', ' ')\n",
    "\n",
    "    # print(art.n_time, art.n_date, art.n_rubr, art.n_diff)\n",
    "    # print(art.n_head)\n",
    "    # print(art.n_author)\n",
    "    # print(art.n_text)\n",
    "    #return [n_time, n_date, n_rubr, n_diff, n_author, n_head, n_text]\n",
    "    return art\n",
    "\n",
    "def getLentaArticle(url):\n",
    "    \"\"\" getLentaArticle gets the body of an article from Lenta.ru\"\"\"\n",
    "    # Получает текст страницы.\n",
    "    resp=requests.get(url)\n",
    "    # Загружаем текст в объект типа BeautifulSoup.\n",
    "    bs=BeautifulSoup(resp.text, \"html5lib\")\n",
    "    art=BaseArticle()\n",
    "    art.title=bs.h1.text.replace(\"\\xa0\", \" \")\n",
    "    art.text=BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")]), \"html5lib\").get_text().replace(\"\\xa0\", \" \")\n",
    "    return art\n",
    "\n",
    "def articleToJSON(art):\n",
    "    # Здесь сделаем неправильно - посмотрим на isinstance(BaseArticle), а потом elif(NPlus1Article), тогда в else не пойдет.\n",
    "    if isinstance(art , BaseArticle):\n",
    "        return '{\"title\":\"'+art.title+art.text.replace('\"', '\\\\\"')+'\"}'\n",
    "    elif isinstance(art , NPlus1Article):\n",
    "        res='{\"date\":\"'+art.date+'\", \"time\":\"'+art.time+'\", \"rubrics\":\"'+art.rubr+'\", \"difficulty\":\"'\n",
    "        res+=art.diff+'\", \"title\":\"'+art.title+'\", \"author\":\"'+art.author+'\",\"text\":\"'\n",
    "        res+=art.text.replace('\"', '\\\\\"')+'\"}'\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем статьи и выводим JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\":\"Германия научит Россию собирать мусорБывший министр Германии по вопросам окружающей среды, охраны природы и защиты атомных реакторов Клаус Тепфер проконсультирует российское Минприроды по поводу утилизации мусора, пишет РБК со ссылкой на пресс-службу вице-премьера Алексея Гордеева. «Главный идеолог немецкой модели переработки и утилизации мусора выступит независимым консультантом запуска российской системы обращения с твердыми коммунальными отходами и будет экспертно сопровождать ход ее реализации», — говорится в сообщении по итогам визита Гордеева в Германию. Отмечается, что во время поездки вице-премьер ознакомился с немецкой системой сбора и переработки мусора. Благодаря ей все отходы удается перерабатывать во вторсырье, «зеленый уголь» и цветочный грунт. В России этот путь проделывают только 10 процентов отходов. По словам Гордеева, российские власти ставят перед собой задачу в несколько раз повысить эффективность переработки отходов за ближайшие шесть лет, а также полностью рекультивировать 200 свалок, расположенных в черте российских городов. Ранее в ходе послания к Федеральному собранию президент Владимир Путин потребовал навести порядок в сфере сбора и переработки мусора. С 2019 года в России проходит реформа, в рамках которой был создан «Российский экологический оператор». В его задачи будет входить: координация работы региональных операторов, финансирование проектов по обработке отходов, а также предоставление гарантий инвестиционным проектам.\"}\n",
      "{\"title\":\"Американские разработчики представили DeepSqeak — программу для автоматической обработки вокализации грызунов. Она работает на основе сверточных нейросетей и позволяет выделять из аудиофайлов отдельные звуки по их сонограмме, после чего классифицирует их, создавая «словарь». Статья опубликована в журнале Neuropsychopharmacology, сам алгоритм доступен в репозитории разработчиков на GitHub.Для коммуникации друг с другом грызуны используют ультразвуковую вокализацию — издают звуки с частотой от 20 килогерц. «Словарь» таких звуков у них достаточно большой: исследователи подразделяют их на слоги в зависимости от длины и частоты, а каждый из них связывают с определенной реакцией животного. К примеру, звуки с частотой 50 килогерц ассоциируются с положительной эмоциональной реакцией лабораторных крыс, а более низкие звуки — с частотой около 20 — с отрицательной. При этом у мышей коммуникационный паттерн отличается.Исследователи из Вашингтонского университета под руководством Кевина Коффи (Kevin Coffey) решили создать алгоритм, который поможет распознавать звуки, издаваемые грызунами, разделять их на слоги и классифицировать. Анализ с помощью DeepSqeak состоит из нескольких этапов: сначала из звукового файла создается сонограмма, из которой выделяются звуковые фрагменты. Так как на сонограмму может попасть и шум, далее в алгоритме используется сверточная нейросеть, которая разделяет звуковые колебания на звуки, которые издают сами грызуны, и сторонний шум. \n",
      "    \n",
      "        \n",
      "                        \n",
      "                \n",
      "                    \n",
      "                        \n",
      "                    \"}\n"
     ]
    }
   ],
   "source": [
    "a1=getLentaArticle(\"https://lenta.ru/news/2019/02/20/trash/\")\n",
    "a2=getArticleTextNPlus1(\"https://nplus1.ru/news/2019/02/20/deep-sqeak\")\n",
    "\n",
    "print(articleToJSON(a1))\n",
    "print(articleToJSON(a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оказывается, функция `isinstance` проверяет не приводится ли объект к проверяемому типу и если приводится, то возвращает True. Нам же необходимо проверить точное совпадение с типом. Для этого будем использовать конструкцию `type(a) is`. \n",
    "\n",
    "Можно, конечно, просто расставить проверки в правильном порядке, оставив базовый класс напоследок, но так больше шанс наделать ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\":\"Германия научит Россию собирать мусорБывший министр Германии по вопросам окружающей среды, охраны природы и защиты атомных реакторов Клаус Тепфер проконсультирует российское Минприроды по поводу утилизации мусора, пишет РБК со ссылкой на пресс-службу вице-премьера Алексея Гордеева. «Главный идеолог немецкой модели переработки и утилизации мусора выступит независимым консультантом запуска российской системы обращения с твердыми коммунальными отходами и будет экспертно сопровождать ход ее реализации», — говорится в сообщении по итогам визита Гордеева в Германию. Отмечается, что во время поездки вице-премьер ознакомился с немецкой системой сбора и переработки мусора. Благодаря ей все отходы удается перерабатывать во вторсырье, «зеленый уголь» и цветочный грунт. В России этот путь проделывают только 10 процентов отходов. По словам Гордеева, российские власти ставят перед собой задачу в несколько раз повысить эффективность переработки отходов за ближайшие шесть лет, а также полностью рекультивировать 200 свалок, расположенных в черте российских городов. Ранее в ходе послания к Федеральному собранию президент Владимир Путин потребовал навести порядок в сфере сбора и переработки мусора. С 2019 года в России проходит реформа, в рамках которой был создан «Российский экологический оператор». В его задачи будет входить: координация работы региональных операторов, финансирование проектов по обработке отходов, а также предоставление гарантий инвестиционным проектам.\"}\n",
      "{\"date\":\"20 Фев. 2019\", \"time\":\"18:55\", \"rubrics\":\"Биология\", \"difficulty\":\"3.1\", \"title\":\"\", \"author\":\"Елизавета Ивтушок\",\"text\":\"Американские разработчики представили DeepSqeak — программу для автоматической обработки вокализации грызунов. Она работает на основе сверточных нейросетей и позволяет выделять из аудиофайлов отдельные звуки по их сонограмме, после чего классифицирует их, создавая «словарь». Статья опубликована в журнале Neuropsychopharmacology, сам алгоритм доступен в репозитории разработчиков на GitHub.Для коммуникации друг с другом грызуны используют ультразвуковую вокализацию — издают звуки с частотой от 20 килогерц. «Словарь» таких звуков у них достаточно большой: исследователи подразделяют их на слоги в зависимости от длины и частоты, а каждый из них связывают с определенной реакцией животного. К примеру, звуки с частотой 50 килогерц ассоциируются с положительной эмоциональной реакцией лабораторных крыс, а более низкие звуки — с частотой около 20 — с отрицательной. При этом у мышей коммуникационный паттерн отличается.Исследователи из Вашингтонского университета под руководством Кевина Коффи (Kevin Coffey) решили создать алгоритм, который поможет распознавать звуки, издаваемые грызунами, разделять их на слоги и классифицировать. Анализ с помощью DeepSqeak состоит из нескольких этапов: сначала из звукового файла создается сонограмма, из которой выделяются звуковые фрагменты. Так как на сонограмму может попасть и шум, далее в алгоритме используется сверточная нейросеть, которая разделяет звуковые колебания на звуки, которые издают сами грызуны, и сторонний шум. \n",
      "    \n",
      "        \n",
      "                        \n",
      "                \n",
      "                    \n",
      "                        \n",
      "                    \"}\n"
     ]
    }
   ],
   "source": [
    "def articleToJSON(art):\n",
    "    # Здесь сделаем неправильно - посмотрим на isinstance(BaseArticle), а потом elif(NPlus1Article), тогда в else не пойдет.\n",
    "    if type(art) is BaseArticle:\n",
    "        return '{\"title\":\"'+art.title+art.text.replace('\"', '\\\\\"')+'\"}'\n",
    "    elif type(art) is NPlus1Article:\n",
    "        res='{\"date\":\"'+art.date+'\", \"time\":\"'+art.time+'\", \"rubrics\":\"'+art.rubr+'\", \"difficulty\":\"'\n",
    "        res+=art.diff+'\", \"title\":\"'+art.title+'\", \"author\":\"'+art.author+'\",\"text\":\"'\n",
    "        res+=art.text.replace('\"', '\\\\\"')+'\"}'\n",
    "        return res\n",
    "\n",
    "a1=getLentaArticle(\"https://lenta.ru/news/2019/02/20/trash/\")\n",
    "a2=getArticleTextNPlus1(\"https://nplus1.ru/news/2019/02/20/deep-sqeak\")\n",
    "\n",
    "print(articleToJSON(a1))\n",
    "print(articleToJSON(a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так работает корректней.\n",
    "\n",
    "Мы увидели, что наследование может использоваться для расширения функционала. Попробуем применить это знание к проектированию классов, которые загружают статьи. Создадим базовый класс, который умеет сохранять статьи в файл и считывать их оттуда. При этом сохраняться будут только заголовки и текст, а остальная информация, если она была, будет теряться. Также класс будет уметь строить частотные словари для статей. Дальше унаследуемся от этого класса и добавим функции работы с заданным сайтом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGetNewsPaper:\n",
    "    ''' Базовый класс для загрузки статей. Обеспечивает основной функционал, но не интерфейс.\n",
    "    '''        \n",
    "    # Конструктор - вызывается при создании объекта и инициализирует его.\n",
    "    def __init__(self):\n",
    "        self.articles=[]     # Загруженные статьи.\n",
    "        self.dictionaries=[] # Словари для каждой из статей.\n",
    "        # Создаем и загружаем морфологический словарь.\n",
    "        self.morph=pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    # Построение вектора для статьи.\n",
    "    def getArticleDictionary(self, text, needPos=None):\n",
    "        words=[a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        reswords=[]\n",
    "    \n",
    "        for w in words:\n",
    "            wordform=self.morph.parse(w)[0]\n",
    "            try:\n",
    "                if wordform.tag.POS in ['ADJF', 'NOUN', 'VERB', 'PRTF', 'GRND']:\n",
    "                    if needPos!=None:\n",
    "                        reswords.append(wordform.normal_form+'_'+wordform.tag.POS)\n",
    "                    else:\n",
    "                        reswords.append(wordform.normal_form)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        stat=Counter(reswords)\n",
    "        # Берем только слова с частотой больше 1.\n",
    "        stat={a: stat[a] for a in stat.keys() if stat[a]>1}\n",
    "        return stat\n",
    "\n",
    "    # Посчитаем вектора для всех статей.\n",
    "    def calcArticleDictionaries(self, needPos=None):\n",
    "        self.dictionaries=[]\n",
    "        for a in self.articles:\n",
    "            self.dictionaries.append(self.getArticleDictionary(a.text, needPos))\n",
    "            \n",
    "    # Сохраняем статьи в файл.\n",
    "    def saveArticles(self, filename):\n",
    "        \"\"\" Saves all articles to a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, \"w\")\n",
    "        for art in self.articles:\n",
    "            newsfile.write('\\n=====\\n'+art.title)\n",
    "            newsfile.write('\\n-----\\n'+art.text)\n",
    "        newsfile.close()\n",
    "\n",
    "    # Читаем статьи из файла.\n",
    "    def loadArticles(self, filename):\n",
    "        \"\"\" Loads and replaces all articles from a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, encoding=\"utf-8\")\n",
    "        text = newsfile.read()\n",
    "        loaded = text.split('\\n=====\\n')[1:]\n",
    "        self.articles=[]\n",
    "        for i, a in enumerate(loaded):\n",
    "            self.articles.append(BaseArticle())\n",
    "            b, self.articles[i].text = a.split('\\n-----\\n')\n",
    "            self.articles[i].title = b\n",
    "        newsfile.close()\n",
    "\n",
    "class GetLenta(BaseGetNewsPaper):\n",
    "    ''' Класс для загрузки Ленты.ру.  Наследуется от BaseGetNewsPaper.\n",
    "    '''\n",
    "    # Загрузка статьи по URL.\n",
    "    def getLentaArticle(self, url):\n",
    "        \"\"\" getLentaArticle gets the body of an article from Lenta.ru\"\"\"\n",
    "        # Получает текст страницы.\n",
    "        resp=requests.get(url)\n",
    "        # Загружаем текст в объект типа BeautifulSoup.\n",
    "        bs=BeautifulSoup(resp.text, \"html5lib\") \n",
    "        \n",
    "        art=BaseArticle()\n",
    "        # Получаем заголовок статьи.\n",
    "        art.title=bs.h1.text.replace(\"\\xa0\", \" \")\n",
    "        # Получаем текст статьи.\n",
    "        art.text=BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")]), \"html5lib\").get_text().replace(\"\\xa0\", \" \")\n",
    "        return art\n",
    "\n",
    "    # Загрузка всех статей за один день.\n",
    "    def getLentaDay(self, url):\n",
    "        \"\"\" Gets all URLs for a given day and gets all texts. \"\"\"\n",
    "        try:\n",
    "            # Грузим страницу со списком всех статей.\n",
    "            day = requests.get(url) \n",
    "            # Получаем фрагменты с нужными нам адресами статей.\n",
    "            h3s=BeautifulSoup(day.text, \"html5lib\").find_all(\"h3\")\n",
    "            # Получаем все адреса на статьи за день.\n",
    "            links=[\"http://lenta.ru\"+l.find_all(\"a\")[0][\"href\"] for l in h3s]\n",
    "            # Загружаем статьи.\n",
    "            for l in links:\n",
    "                art=self.getLentaArticle(l)\n",
    "                self.articles.append(art)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getLentaPeriod(self, start, finish):\n",
    "        \"\"\"Gets articles for a period fom start to finish. \"\"\"\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getLentaDay('https://lenta.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "class GetNPlus1(BaseGetNewsPaper):\n",
    "    ''' Класс для загрузки NPlus1.ru. Наследуется от BaseGetNewsPaper.\n",
    "    '''\n",
    "    def getArticleTextNPlus1(self, adr):\n",
    "        \"\"\"Get an article from nplus1.ru\"\"\"\n",
    "        r = requests.get(adr)\n",
    "        #print(r.text)\n",
    "        art = NPlus1Article()\n",
    "        tables = re.split(\"</div>\",re.split('=\"tables\"', r.text)[1])[0]\n",
    "        t1 = re.split(\"</time>\", re.split(\"<time\", tables)[1])[0]\n",
    "        art.time = re.split(\"</span>\", re.split(\"<span>\", t1)[1])[0]\n",
    "        art.date = re.split(\"</span>\", re.split(\"<span>\", t1)[2])[0]\n",
    "        art.rubr = re.findall(\"<a data-rubric.+?>(.+?)</a>\", r.text)[0]\n",
    "        art.diff = re.split(\"</span>\", re.split('\"difficult-value\">', tables)[1])[0]\n",
    "        art.title = re.findall(\"<h1>(.+?)</h1>\", r.text)[0]\n",
    "        art.author = re.split('\" />',re.split('<meta name=\"author\" content=\"', r.text)[1])[0]\n",
    "        art.text = re.split(\"</div>\", re.split(\"</figure>\", re.split('</article>',re.split('<article', r.text)[1])[0])[1])[1]    \n",
    "\n",
    "        beaux_text = BeautifulSoup(art.text, \"html5lib\")\n",
    "        art.text = delcom.sub(\"\", beaux_text.get_text() )\n",
    "        art.text = art.text.replace('\\xa0', ' ')\n",
    "        return art\n",
    "\n",
    "    def getNPlus1Day(self, adr):\n",
    "        \"\"\" Gwt all article for a day by its URL given in adr parameter.\"\"\"\n",
    "        r = requests.get(adr)\n",
    "        titles = BeautifulSoup(r.text, \"html5lib\")(\"article\")\n",
    "        addrs = [\"https://nplus1.ru/\"+a(\"a\")[0][\"href\"] for a in titles]\n",
    "        for adr in addrs:\n",
    "            aa=self.getArticleTextNPlus1(adr)\n",
    "            self.articles.append(aa)\n",
    "        \n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getNPlus1Period(self, start, finish):\n",
    "        \"\"\" Gets all articles from nplus1.ru for a period from start to finish. \"\"\"\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getNPlus1Day('https://nplus1.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем как работают наши классы - загрузим три дня с обоих сайтов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/02/01\n",
      "2018/02/02\n",
      "2018/02/03\n"
     ]
    }
   ],
   "source": [
    "lenta=GetLenta()\n",
    "lenta.getLentaPeriod(datetime.date(2018, 2, 1), datetime.date(2018, 2, 3))\n",
    "lenta.saveArticles(\"data/lenta_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/02/01\n",
      "2018/02/02\n",
      "2018/02/03\n"
     ]
    }
   ],
   "source": [
    "n1=GetNPlus1()\n",
    "n1.getNPlus1Period(datetime.date(2018, 2, 1), datetime.date(2018, 2, 3))\n",
    "\n",
    "n1.saveArticles(\"data/nplus1_test.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем статьи из файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ДНК помогла управлять роем молекулярных моторов из микротрубочек'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2=GetNPlus1()\n",
    "n2.loadArticles(\"data/nplus1_test.txt\")\n",
    "\n",
    "n2.articles[1].title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако давать разные названия для функций, которые делают одно и то же - не очень хорошо, особенно если эти функции входят в интерфейс класса.\n",
    "\n",
    "Интерфейс (в случае Python) - это некоторые обязательства, что класс умеет выполнять определенные функции. В этом случае не важно, какой именно класс наследует. Для этого необходимо у базового класса определить необходимые функции, а в дочерних классах переопределить эти функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGetNewsPaper:\n",
    "    ''' Базовый класс для загрузки статей. Обеспечивает основной функционал и интерфейс.\n",
    "        Классы-наследники будут уметь загружать данные при помощи одного программного интерфейса.\n",
    "    '''        \n",
    "        \n",
    "    # Конструктор - вызывается при создании объекта и инициализирует его.\n",
    "    def __init__(self):\n",
    "        self.articles=[]     # Загруженные статьи.\n",
    "        self.dictionaries=[] # Словари для каждой из статей.\n",
    "        # Создаем и загружаем морфологический словарь.\n",
    "        self.morph=pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    # Построение вектора для статьи.\n",
    "    def getArticleDictionary(self, text, needPos=None):\n",
    "        words=[a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        reswords=[]\n",
    "    \n",
    "        for w in words:\n",
    "            wordform=self.morph.parse(w)[0]\n",
    "            try:\n",
    "                if wordform.tag.POS in ['ADJF', 'NOUN', 'VERB', 'PRTF', 'GRND']:\n",
    "                    if needPos!=None:\n",
    "                        reswords.append(wordform.normal_form+'_'+wordform.tag.POS)\n",
    "                    else:\n",
    "                        reswords.append(wordform.normal_form)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        stat=Counter(reswords)\n",
    "        # Берем только слова с частотой больше 1.\n",
    "        stat={a: stat[a] for a in stat.keys() if stat[a]>1}\n",
    "        return stat\n",
    "\n",
    "    # Посчитаем вектора для всех статей.\n",
    "    def calcArticleDictionaries(self, needPos=None):\n",
    "        self.dictionaries=[]\n",
    "        for a in self.articles:\n",
    "            self.dictionaries.append(self.getArticleDictionary(a.text, needPos))\n",
    "            \n",
    "\n",
    "    def getPeriod(self, start, finish):\n",
    "        print(\"Nothing to do.\")\n",
    "    \n",
    "    # Сохраняем статьи в файл.\n",
    "    def saveArticles(self, filename):\n",
    "        \"\"\" Saves all articles to a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, \"w\")\n",
    "        for art in self.articles:\n",
    "            newsfile.write('\\n=====\\n'+art.title)\n",
    "            newsfile.write('\\n-----\\n'+art.text)\n",
    "        newsfile.close()\n",
    "\n",
    "    # Читаем статьи из файла.\n",
    "    def loadArticles(self, filename):\n",
    "        \"\"\" Loads and replaces all articles from a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, encoding=\"utf-8\")\n",
    "        text = newsfile.read()\n",
    "        loaded = text.split('\\n=====\\n')[1:]\n",
    "        self.articles=[]\n",
    "        for i, a in enumerate(loaded):\n",
    "            self.articles.append(BaseArticle())\n",
    "            b, self.articles[i].text = a.split('\\n-----\\n')\n",
    "            self.articles[i].title = b\n",
    "        newsfile.close()\n",
    "        \n",
    "class GetLenta(BaseGetNewsPaper):\n",
    "    ''' Класс для загрузки Ленты.ру.  Наследуется от BaseGetNewsPaper.\n",
    "    '''\n",
    "\n",
    "    # Загрузка статьи по URL.\n",
    "    def getLentaArticle(self, url):\n",
    "        \"\"\" getLentaArticle gets the body of an article from Lenta.ru\"\"\"\n",
    "        # Получает текст страницы.\n",
    "        resp=requests.get(url)\n",
    "        # Загружаем текст в объект типа BeautifulSoup.\n",
    "        bs=BeautifulSoup(resp.text, \"html5lib\") \n",
    "        \n",
    "        art=BaseArticle()\n",
    "        # Получаем заголовок статьи.\n",
    "        art.title=bs.h1.text.replace(\"\\xa0\", \" \")\n",
    "        # Получаем текст статьи.\n",
    "        art.text=BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")]), \"html5lib\").get_text().replace(\"\\xa0\", \" \")\n",
    "        return art\n",
    "\n",
    "    # Загрузка всех статей за один день.\n",
    "    def getLentaDay(self, url):\n",
    "        \"\"\" Gets all URLs for a given day and gets all texts. \"\"\"\n",
    "        try:\n",
    "            # Грузим страницу со списком всех статей.\n",
    "            day = requests.get(url) \n",
    "            # Получаем фрагменты с нужными нам адресами статей.\n",
    "            h3s=BeautifulSoup(day.text, \"html5lib\").find_all(\"h3\")\n",
    "            # Получаем все адреса на статьи за день.\n",
    "            links=[\"http://lenta.ru\"+l.find_all(\"a\")[0][\"href\"] for l in h3s]\n",
    "            # Загружаем статьи.\n",
    "            for l in links:\n",
    "                art=self.getLentaArticle(l)\n",
    "                self.articles.append(art)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getLentaPeriod(self, start, finish):\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getLentaDay('https://lenta.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "    def getPeriod(self, start, finish):\n",
    "        self.getLentaPeriod(start, finish)\n",
    "    \n",
    "\n",
    "class GetNPlus1(BaseGetNewsPaper):\n",
    "    ''' Класс для загрузки NPlus1.ru.  Наследуется от BaseGetNewsPaper.\n",
    "    '''\n",
    "\n",
    "    def getArticleTextNPlus1(self, adr):\n",
    "        r = requests.get(adr)\n",
    "        #print(r.text)\n",
    "        art = NPlus1Article()\n",
    "        tables = re.split(\"</div>\",re.split('=\"tables\"', r.text)[1])[0]\n",
    "        t1 = re.split(\"</time>\", re.split(\"<time\", tables)[1])[0]\n",
    "        art.time = re.split(\"</span>\", re.split(\"<span>\", t1)[1])[0]\n",
    "        art.date = re.split(\"</span>\", re.split(\"<span>\", t1)[2])[0]\n",
    "        art.rubr = re.findall(\"<a data-rubric.+?>(.+?)</a>\", r.text)[0]\n",
    "        art.diff = re.split(\"</span>\", re.split('\"difficult-value\">', tables)[1])[0]\n",
    "        art.title = re.findall(\"<h1>(.+?)</h1>\", r.text)[0]\n",
    "        art.author = re.split('\" />',re.split('<meta name=\"author\" content=\"', r.text)[1])[0]\n",
    "        art.text = re.split(\"</div>\", re.split(\"</figure>\", re.split('</article>',re.split('<article', r.text)[1])[0])[1])[1]    \n",
    "\n",
    "        beaux_text = BeautifulSoup(art.text, \"html5lib\")\n",
    "        art.text = delcom.sub(\"\", beaux_text.get_text() )\n",
    "        art.text = art.text.replace('\\xa0', ' ')\n",
    "        return art\n",
    "\n",
    "    def getNPlus1Day(self, adr):\n",
    "        r = requests.get(adr)\n",
    "        titles = BeautifulSoup(r.text, \"html5lib\")(\"article\")\n",
    "        addrs = [\"https://nplus1.ru/\"+a(\"a\")[0][\"href\"] for a in titles]\n",
    "        for adr in addrs:\n",
    "            aa=self.getArticleTextNPlus1(adr)\n",
    "            self.articles.append(aa)\n",
    "        \n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getNPlus1Period(self, start, finish):\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getNPlus1Day('https://nplus1.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "    def getPeriod(self, start, finish):\n",
    "        self.getNPlus1Period(start, finish)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n1=GetNPlus1()\n",
    "#filename=\"nplus1_test.txt\"\n",
    "n1=GetLenta()\n",
    "filename=\"lenta_test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь попробуйте угадать для какого класса был выполнен этот код?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/02/01\n",
      "2018/02/02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n1.getPeriod(datetime.date(2018, 2, 1), datetime.date(2018, 2, 2))\n",
    "n1.saveArticles(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Абстрактные методы и классы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем так, чтобы объекты базового класса нельзя было создавать. Это не совсем хорошо именно для нашего случая, но для учебных целей нормально.\n",
    "\n",
    "Подключим абстрактные классы из библиотеки ABC (Abstract Base Classes) и декоратор abstractmethod. Теперь метод `getPeriod` будет абстрактным, то есть он не реализован. Из-за этого (и наследования от ABC) класс BaseGetNewsPaper будет абстрактным, то есть его объекты нельзя создавать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGetNewsPaper(ABC):\n",
    "        \n",
    "    # Конструктор - вызывается при создании объекта и инициализирует его.\n",
    "    def __init__(self):\n",
    "        self.articles=[]     # Загруженные статьи.\n",
    "        self.dictionaries=[] # Словари для каждой из статей.\n",
    "        # Создаем и загружаем морфологический словарь.\n",
    "        self.morph=pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    # Построение вектора для статьи.\n",
    "    def getArticleDictionary(self, text, needPos=None):\n",
    "        words=[a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        reswords=[]\n",
    "    \n",
    "        for w in words:\n",
    "            wordform=self.morph.parse(w)[0]\n",
    "            try:\n",
    "                if wordform.tag.POS in ['ADJF', 'NOUN', 'VERB', 'PRTF', 'GRND']:\n",
    "                    if needPos!=None:\n",
    "                        reswords.append(wordform.normal_form+'_'+wordform.tag.POS)\n",
    "                    else:\n",
    "                        reswords.append(wordform.normal_form)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        stat=Counter(reswords)\n",
    "        # Берем только слова с частотой больше 1.\n",
    "        stat={a: stat[a] for a in stat.keys() if stat[a]>1}\n",
    "        return stat\n",
    "\n",
    "    # Посчитаем вектора для всех статей.\n",
    "    def calcArticleDictionaries(self, needPos=None):\n",
    "        self.dictionaries=[]\n",
    "        for a in self.articles:\n",
    "            self.dictionaries.append(self.getArticleDictionary(a.text, needPos))\n",
    "            \n",
    "    @abstractmethod\n",
    "    def getPeriod(self, start, finish):\n",
    "        pass\n",
    "    \n",
    "    # Сохраняем статьи в файл.\n",
    "    def saveArticles(self, filename):\n",
    "        \"\"\" Saves all articles to a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, \"w\")\n",
    "        for art in self.articles:\n",
    "            newsfile.write('\\n=====\\n'+art.title)\n",
    "            newsfile.write('\\n-----\\n'+art.text)\n",
    "        newsfile.close()\n",
    "\n",
    "    # Читаем статьи из файла.\n",
    "    def loadArticles(self, filename):\n",
    "        \"\"\" Loads and replaces all articles from a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, encoding=\"utf-8\")\n",
    "        text = newsfile.read()\n",
    "        loaded = text.split('\\n=====\\n')[1:]\n",
    "        self.articles=[]\n",
    "        for i, a in enumerate(loaded):\n",
    "            self.articles.append(BaseArticle())\n",
    "            b, self.articles[i].text = a.split('\\n-----\\n')\n",
    "            self.articles[i].title = b\n",
    "        newsfile.close()\n",
    "        \n",
    "class GetLenta(BaseGetNewsPaper):\n",
    "    # Загрузка статьи по URL.\n",
    "    def getLentaArticle(self, url):\n",
    "        \"\"\" getLentaArticle gets the body of an article from Lenta.ru\"\"\"\n",
    "        # Получает текст страницы.\n",
    "        resp=requests.get(url)\n",
    "        # Загружаем текст в объект типа BeautifulSoup.\n",
    "        bs=BeautifulSoup(resp.text, \"html5lib\") \n",
    "        \n",
    "        art=BaseArticle()\n",
    "        # Получаем заголовок статьи.\n",
    "        art.title=bs.h1.text.replace(\"\\xa0\", \" \")\n",
    "        # Получаем текст статьи.\n",
    "        art.text=BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")]), \"html5lib\").get_text().replace(\"\\xa0\", \" \")\n",
    "        return art\n",
    "\n",
    "    # Загрузка всех статей за один день.\n",
    "    def getLentaDay(self, url):\n",
    "        \"\"\" Gets all URLs for a given day and gets all texts. \"\"\"\n",
    "        try:\n",
    "            # Грузим страницу со списком всех статей.\n",
    "            day = requests.get(url) \n",
    "            # Получаем фрагменты с нужными нам адресами статей.\n",
    "            h3s=BeautifulSoup(day.text, \"html5lib\").find_all(\"h3\")\n",
    "            # Получаем все адреса на статьи за день.\n",
    "            links=[\"http://lenta.ru\"+l.find_all(\"a\")[0][\"href\"] for l in h3s]\n",
    "            # Загружаем статьи.\n",
    "            for l in links:\n",
    "                art=self.getLentaArticle(l)\n",
    "                self.articles.append(art)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getLentaPeriod(self, start, finish):\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getLentaDay('https://lenta.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "    def getPeriod(self, start, finish):\n",
    "        self.getLentaPeriod(start, finish)\n",
    "    \n",
    "\n",
    "class GetNPlus1(BaseGetNewsPaper):\n",
    "    def getArticleTextNPlus1(self, adr):\n",
    "        r = requests.get(adr)\n",
    "        #print(r.text)\n",
    "        art = NPlus1Article()\n",
    "        tables = re.split(\"</div>\",re.split('=\"tables\"', r.text)[1])[0]\n",
    "        t1 = re.split(\"</time>\", re.split(\"<time\", tables)[1])[0]\n",
    "        art.time = re.split(\"</span>\", re.split(\"<span>\", t1)[1])[0]\n",
    "        art.date = re.split(\"</span>\", re.split(\"<span>\", t1)[2])[0]\n",
    "        art.rubr = re.findall(\"<a data-rubric.+?>(.+?)</a>\", r.text)[0]\n",
    "        art.diff = re.split(\"</span>\", re.split('\"difficult-value\">', tables)[1])[0]\n",
    "        art.title = re.findall(\"<h1>(.+?)</h1>\", r.text)[0]\n",
    "        art.author = re.split('\" />',re.split('<meta name=\"author\" content=\"', r.text)[1])[0]\n",
    "        art.text = re.split(\"</div>\", re.split(\"</figure>\", re.split('</article>',re.split('<article', r.text)[1])[0])[1])[1]    \n",
    "\n",
    "        beaux_text = BeautifulSoup(art.text, \"html5lib\")\n",
    "        art.text = delcom.sub(\"\", beaux_text.get_text() )\n",
    "        art.text = art.text.replace('\\xa0', ' ')\n",
    "        return art\n",
    "\n",
    "    def getNPlus1Day(self, adr):\n",
    "        r = requests.get(adr)\n",
    "        titles = BeautifulSoup(r.text, \"html5lib\")(\"article\")\n",
    "        addrs = [\"https://nplus1.ru/\"+a(\"a\")[0][\"href\"] for a in titles]\n",
    "        for adr in addrs:\n",
    "            aa=self.getArticleTextNPlus1(adr)\n",
    "            self.articles.append(aa)\n",
    "        \n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getNPlus1Period(self, start, finish):\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getNPlus1Day('https://nplus1.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "    def getPeriod(self, start, finish):\n",
    "        self.getNPlus1Period(start, finish)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class BaseGetNewsPaper with abstract methods getPeriod",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-68995f780023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Создать не получится - теперь это абстрактный класс.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBaseGetNewsPaper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class BaseGetNewsPaper with abstract methods getPeriod"
     ]
    }
   ],
   "source": [
    "# Создать не получится - теперь это абстрактный класс.\n",
    "nn=BaseGetNewsPaper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А здесь всё в порядке.\n",
    "n1=GetLenta()\n",
    "n2=GetNPlus1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1.getPeriod(datetime.date(2018, 2, 1), datetime.date(2018, 2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим этим классам набор операторов, которые сделают работу ними более удобной. Хотя некоторые операторы скорее для демонстрации возможностей применения перегруженных операторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовый класс статьи, он же класс статьи для Ленты.ру\n",
    "class BaseArticle:\n",
    "    def __init__(self, _title=None, _text=None):\n",
    "        \"\"\"Конструктор для базового класса статьи. Заводит поля title и text.\"\"\"\n",
    "        if isinstance(_title, str) and isinstance(_text, str):\n",
    "            self.title=_title\n",
    "            self.text=_text\n",
    "        else:\n",
    "            self.title=\"\"\n",
    "            self.text=\"\"\n",
    "        \n",
    "    # Конвертация в JSON.\n",
    "    def toJSON(self):\n",
    "        \"\"\"Возвращает представление базовой статьи в формате строки JSON.\"\"\"\n",
    "        res='{\"title\":\"'+self.title.replace('\"', '\\\\\"')+'\",\"text\":\"'+self.text.replace('\"', '\\\\\"')+'\"}'\n",
    "        return res\n",
    "\n",
    "    # Конвертация в словарь.\n",
    "    def toDict(self):\n",
    "        \"\"\"Возвращает представление базовой статьи в виде словаря.\"\"\"\n",
    "        res={\"title\":self.title.replace('\"', '\\\\\"'), \"text\":self.text.replace('\"', '\\\\\"')}\n",
    "        return res\n",
    "    \n",
    "    # Возвращает строковое представление статьи если преобразуется к строке при помощи str(article)\n",
    "    # или print(article).\n",
    "    def __str__(self):\n",
    "        \"\"\"Возвращает строку из 200 первых символов заголовка статьи и 200 первых символов самой статьи.\"\"\"\n",
    "        return '<title: '+self.title[:200]+'\\ntext: '+self.text[:200]+'... >'\n",
    "\n",
    "    # Возвращает строковое представление статьи если мы просим среду отобразить статью ьез print.\n",
    "    def __repr__(self):\n",
    "        \"\"\"Возвращает строку из 100 первых символов заголовка как краткое текстовое представление статьи.\"\"\"\n",
    "        return '<Base Article on \"'+self.title[:100]+'\">'\n",
    "    \n",
    "    \n",
    "class NPlus1Article(BaseArticle):\n",
    "    def __init__(self):\n",
    "        \"\"\"Конструктор для базового класса статьи. Заводит поля title, text, date, time, rubr, diff и author.\"\"\"\n",
    "        # Вызываем конструктор от базового класса.\n",
    "        super().__init__()\n",
    "        self.time=\"\"\n",
    "        self.date=\"\"\n",
    "        self.rubr=\"\"\n",
    "        self.diff=\"\"\n",
    "        self.author=\"\"\n",
    "        \n",
    "    # Конвертация в JSON.\n",
    "    def toJSON(self):\n",
    "        \"\"\"Возвращает представление базовой статьи в формате строки JSON.\"\"\"\n",
    "        res='{\"date\":\"'+self.date+'\", \"time\":\"'+self.time+'\", \"rubrics\":\"'+self.rubr+'\", \"difficulty\":\"'\n",
    "        res+=self.diff+'\", \"title\":\"'+self.title+'\", \"author\":\"'+self.author+'\",\"text\":\"'\n",
    "        res+=self.text.replace('\"', '\\\\\"')+'\"}'\n",
    "        return res\n",
    "\n",
    "    # Конвертация в словарь.\n",
    "    def toDict(self):\n",
    "        \"\"\"Возвращает представление базовой статьи в виде словаря.\"\"\"\n",
    "        res={\"date\":self.date, \"time\":self.time, \"rubrics\":self.rubr, \"difficulty\":self.diff,\\\n",
    "             \"title\":self.title, \"author\":self.author,\"text\":self.text.replace('\"', '\\\\\"')}\n",
    "        return res\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Возвращает строку из метаданных о статье, 200 первых символов заголовка статьи \n",
    "           и 200 первых символов самой статьи.\"\"\"\n",
    "        res='<date:\"'+self.date+' : '+self.time+'\\nrubrics: '+self.rubr+'\\ndifficulty: '+self.diff+ \\\n",
    "            '\\nauthor: '+self.author\n",
    "        res+='\\ntitle: '+self.title[:100]+'\\ntext: '+self.text[:100]+'>'\n",
    "        return res\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Возвращает строку из 100 первых символов заголовка как краткое текстовое представление статьи.\"\"\"\n",
    "        return '<NPlus1 Article on \"'+self.title[:100]+'\">'\n",
    "    \n",
    "    \n",
    "# Базовый класс для загрузчиков новостей.\n",
    "# В образовательных целях сделан как абстрактный класс, то есть класс, объекты которого нельзя создавать.\n",
    "# Можно унаследоваться, переопределить абстрактные функции (отмечены декоратором @abstractmethod).\n",
    "# По-хорошему, можно было бы использовать для того, чтобы прочитать новости и работать с ними.\n",
    "# Умеет посчитать частотные вектора статей.\n",
    "class BaseGetNewsPaper(ABC):\n",
    "        \n",
    "    # Конструктор - вызывается при создании объекта и инициализирует его.\n",
    "    def __init__(self, data=None):\n",
    "        \"\"\"Конструктор объектов класса BaseGetNewsPaper. \n",
    "           Может принимать инициализирующие параметры типа BaseGetNewsPaper (создает копию)\n",
    "           и list (в этом случае оставляет заголовки пустыми).\"\"\"\n",
    "        # Проверяем тип переданного параметра и в зависимости от него по-разному инициализируем объект.\n",
    "        if data==None:\n",
    "            self.articles=[]\n",
    "            self.dictionaries=[]\n",
    "        if isinstance(data, BaseGetNewsPaper):\n",
    "            self.articles=copy(data.articles)     \n",
    "            self.dictionaries=copy(data.dictionaries)\n",
    "        elif isinstance(data, list):\n",
    "            self.articles=copy(data)\n",
    "            self.dictionaries=[]\n",
    "        # В любом случае создаем объект морфологии для создания частотных векторов.\n",
    "        self.__morph=pymorphy2.MorphAnalyzer()\n",
    "        self.__ttt=1\n",
    "\n",
    "    # Построение вектора для статьи.\n",
    "    def getArticleDictionary(self, text, needPos=None):\n",
    "        \"\"\"Строит частотные векторы для текста статьи, берет только значимые части речи.\"\"\"\n",
    "        words=[a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        reswords=[]\n",
    "    \n",
    "        for w in words:\n",
    "            wordform=self.morph.parse(w)[0]\n",
    "            try:\n",
    "                if wordform.tag.POS in ['ADJF', 'NOUN', 'VERB', 'PRTF', 'GRND']:\n",
    "                    if needPos!=None:\n",
    "                        reswords.append(wordform.normal_form+'_'+wordform.tag.POS)\n",
    "                    else:\n",
    "                        reswords.append(wordform.normal_form)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        stat=Counter(reswords)\n",
    "        # Берем только слова с частотой больше 1.\n",
    "        stat={a: stat[a] for a in stat.keys() if stat[a]>1}\n",
    "        return stat\n",
    "\n",
    "    # Посчитаем вектора для всех статей.\n",
    "    def calcArticleDictionaries(self, needPos=None):\n",
    "        \"\"\"Строит частотные вектора для всех статей в коллекции.\n",
    "           !!! Не ясно что делать, когда пополняем. \n",
    "               По-хорошему надо хранить свойство, которое показывает \n",
    "               надо ли их считать для всех статей при добавлении или нет. !!!\"\"\"\n",
    "        self.dictionaries=[]\n",
    "        for a in self.articles:\n",
    "            self.dictionaries.append(self.getArticleDictionary(a.text, needPos))\n",
    "            \n",
    "    # Абстрактный метод для загрузки новостей за заданный период.\n",
    "    # Должен быть реализован в дочернем классе.\n",
    "    @abstractmethod\n",
    "    def getPeriod(self, start, finish):\n",
    "        \"\"\"Абстрактный метод для загрузки новостей за заданный период.\n",
    "           Должен быть реализован в дочернем классе.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Абстрактный метод для загрузки одной новости по ее адресу.\n",
    "    # Должен быть реализован в дочернем классе.\n",
    "    @abstractmethod\n",
    "    def getArticle(self, url):\n",
    "        \"\"\"Абстрактный метод для загрузки одной новости по ее адресу.\n",
    "        Должен быть реализован в дочернем классе.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Сохраняем статьи в файл.\n",
    "    def saveArticles(self, filename):\n",
    "        \"\"\"Сохраняет статью в файл с именем filename. \n",
    "           Статьи отделены друг от друга строкой \"=====\", заголовок от статьи строкой \"-----\". \"\"\"\n",
    "        newsfile = open(filename, \"w\")\n",
    "        for art in self.articles:\n",
    "            newsfile.write('\\n=====\\n'+art.title)\n",
    "            newsfile.write('\\n-----\\n'+art.text)\n",
    "        newsfile.close()\n",
    "\n",
    "    # Читаем статьи из файла.\n",
    "    def loadArticles(self, filename):\n",
    "        \"\"\" Loads and replaces all articles from a file with a filename. \"\"\"\n",
    "        newsfile = open(filename, encoding=\"utf-8\")\n",
    "        text = newsfile.read()\n",
    "        loaded = text.split('\\n=====\\n')[1:]\n",
    "        self.articles=[]\n",
    "        for i, a in enumerate(loaded):\n",
    "            self.articles.append(BaseArticle())\n",
    "            b, self.articles[i].text = a.split('\\n-----\\n')\n",
    "            self.articles[i].title = b\n",
    "        newsfile.close()\n",
    "        \n",
    "    # Показывает сколько статей загружено.\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    # Возвращает статью, если передано целое число или хранилище статей, если передан slice.\n",
    "    def __getitem__(self, index):\n",
    "        if type(index)==slice:\n",
    "            return type(self)(self.articles[index])\n",
    "        else:\n",
    "            return self.articles[index]\n",
    "        \n",
    "    def __getattr__(self, prop):\n",
    "        if len(prop)==1:\n",
    "            if prop==prop.lower():\n",
    "                return self.articles[ord(prop)-ord('a')].text\n",
    "            else:\n",
    "                return self.articles[ord(prop)-ord('A')].title\n",
    "            \n",
    "    def __lshift__(self, art):\n",
    "        # Здесь надо что-то делать со словарями.\n",
    "        if isinstance(art, BaseArticle):\n",
    "            self.articles.append(art)\n",
    "        elif isinstance(art, str):\n",
    "            a1=BaseArticle()\n",
    "            a1.text=art\n",
    "            a1.title=\"No Title\"\n",
    "            self.articles.append(a1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Should be String or BaseArticle\")\n",
    "        return self\n",
    "        \n",
    "    def __iadd__(self, art):\n",
    "        return self<<art\n",
    "    \n",
    "    def __add__(self, art):\n",
    "        t=type(self)(self) # mtype=type(self), t=mtype(), t=self\n",
    "        t+=art\n",
    "        return t\n",
    "\n",
    "    def __radd__(self, art):\n",
    "        t=type(self)(self)\n",
    "        t+=art\n",
    "        return t\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __str__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for art in self.articles:\n",
    "            yield art\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def morph(self):\n",
    "        return self.__morph\n",
    "        \n",
    "class GetLenta(BaseGetNewsPaper):\n",
    "    # Загрузка статьи по URL.\n",
    "    def getLentaArticle(self, url):\n",
    "        \"\"\" getLentaArticle gets the body of an article from Lenta.ru\"\"\"\n",
    "        # Получает текст страницы.\n",
    "        resp=requests.get(url)\n",
    "        # Загружаем текст в объект типа BeautifulSoup.\n",
    "        bs=BeautifulSoup(resp.text, \"html5lib\") \n",
    "        \n",
    "        art=BaseArticle()\n",
    "        # Получаем заголовок статьи.\n",
    "        art.title=bs.h1.text.replace(\"\\xa0\", \" \")\n",
    "        # Получаем текст статьи.\n",
    "        art.text=BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")]), \"html5lib\").get_text().replace(\"\\xa0\", \" \")\n",
    "        return art\n",
    "\n",
    "    # Загрузка всех статей за один день.\n",
    "    def getLentaDay(self, url):\n",
    "        \"\"\" Gets all URLs for a given day and gets all texts. \"\"\"\n",
    "        try:\n",
    "            # Грузим страницу со списком всех статей.\n",
    "            day = requests.get(url) \n",
    "            # Получаем фрагменты с нужными нам адресами статей.\n",
    "            h3s=BeautifulSoup(day.text, \"html5lib\").find_all(\"h3\")\n",
    "            # Получаем все адреса на статьи за день.\n",
    "            links=[\"http://lenta.ru\"+l.find_all(\"a\")[0][\"href\"] for l in h3s]\n",
    "            # Загружаем статьи.\n",
    "            for l in links:\n",
    "                art=self.getLentaArticle(l)\n",
    "                self.articles.append(art)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getLentaPeriod(self, start, finish):\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getLentaDay('https://lenta.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "    def getPeriod(self, start, finish):\n",
    "        self.getLentaPeriod(start, finish)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"<Lenta.ru scrapper: \"+str(len(self.articles))+\" articles loaded>\"\n",
    "    \n",
    "    def getArticle(self, url):\n",
    "        if \"lenta.ru\" in url.lower():\n",
    "            self.lower().getLentaArticle(url)\n",
    "        else:\n",
    "            raise NotImplementedError(\"I can download from Lenta.ru site only.\")\n",
    "            \n",
    "\n",
    "class GetNPlus1(BaseGetNewsPaper):\n",
    "    def getArticleTextNPlus1(self, adr):\n",
    "        r = requests.get(adr)\n",
    "        #print(r.text)\n",
    "        art = NPlus1Article()\n",
    "        tables = re.split(\"</div>\",re.split('=\"tables\"', r.text)[1])[0]\n",
    "        t1 = re.split(\"</time>\", re.split(\"<time\", tables)[1])[0]\n",
    "        art.time = re.split(\"</span>\", re.split(\"<span>\", t1)[1])[0]\n",
    "        art.date = re.split(\"</span>\", re.split(\"<span>\", t1)[2])[0]\n",
    "        art.rubr = re.findall(\"<a data-rubric.+?>(.+?)</a>\", r.text)[0]\n",
    "        art.diff = re.split(\"</span>\", re.split('\"difficult-value\">', tables)[1])[0]\n",
    "        art.title = re.findall(\"<h1>(.+?)</h1>\", r.text)[0]\n",
    "        art.author = re.split('\" />',re.split('<meta name=\"author\" content=\"', r.text)[1])[0]\n",
    "        art.text = re.split(\"</div>\", re.split(\"</figure>\", re.split('</article>',re.split('<article', r.text)[1])[0])[1])[1]    \n",
    "\n",
    "        beaux_text = BeautifulSoup(art.text, \"html5lib\")\n",
    "        art.text = delcom.sub(\"\", beaux_text.get_text() )\n",
    "        art.text = art.text.replace('\\xa0', ' ')\n",
    "        return art\n",
    "\n",
    "    def getNPlus1Day(self, adr):\n",
    "        r = requests.get(adr)\n",
    "        titles = BeautifulSoup(r.text, \"html5lib\")(\"article\")\n",
    "        addrs = [\"https://nplus1.ru/\"+a(\"a\")[0][\"href\"] for a in titles]\n",
    "        for adr in addrs:\n",
    "            aa=self.getArticleTextNPlus1(adr)\n",
    "            self.articles.append(aa)\n",
    "        \n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getNPlus1Period(self, start, finish):\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getNPlus1Day('https://nplus1.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "    def getPeriod(self, start, finish):\n",
    "        self.getNPlus1Period(start, finish)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"<NPlus1.ru scrapper: \"+str(len(self.articles))+\" articles loaded>\"\n",
    "\n",
    "    def getArticle(self, url):\n",
    "        if \"nplus1.ru\" in url.lower():\n",
    "            return self.getArticleTextNPlus1(url)\n",
    "        else:\n",
    "            raise NotImplementedError(\"I can download from Lenta.ru site only.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title: 123\n",
      "text: ... > <title: 321\n",
      "text: ... >\n",
      "<title: \n",
      "text: ... >\n"
     ]
    }
   ],
   "source": [
    "a1=BaseArticle()\n",
    "a2=BaseArticle()\n",
    "a1.title=\"123\"\n",
    "a2.title=\"321\"\n",
    "print(a1, a2)\n",
    "a3=BaseArticle(a1, a2)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title: Беспилотные автомобили Waymo оказались самыми самостоятельными\n",
      "text: Беспилотные автомобили компании Waymo оказались самыми самостоятельными в 2017 году — в среднем водителям-испытателям приходилось перехватывать управление один раз в почти девять тысяч километров, в т... > \n",
      "\n",
      "<title: Неупорядоченная структура шелка сделала его блестящим и холодным\n",
      "text: Физики обнаружили, что блеск шелка возникает из-за наличия в нитях неупорядоченных полостей нанометровой толщины,на которых происходит интерференция света. Та же причина объясняет и теплообмен в нитях... > \n",
      "\n",
      "<title: Беспилотный автомобиль испытают британскими дорогами\n",
      "text: Автомобильные компании Nissan, Renault и Mitsubishi совместно с Университетом Крэнфилда и управляющей компанией Highways England объявили о намерении провести испытания беспилотного автомобиля левосто... > \n",
      "\n",
      "<title: CRISPR заставит биться сердца больных мышечной дистрофией Дюшенна\n",
      "text: Исследователи отредактировали клетки сердечной мышцы с мутациями, приводящими к развитию миодистрофии Дюшенна. При помощи системы CRISPR-Cas9 мутантные участки гена «выбросили»\n",
      "из мРНК, а из клеток с ... > \n",
      "\n",
      "<title: Потребительские дроны получат радары для уклонения от столкновений\n",
      "text: Американские компании Aurora Flight Sciences и Socionext занялись совместной разработкой радарной системы уклонения от столкновений для небольших потребительских дронов. Согласно сообщению Aurora Flig... > \n",
      "\n",
      "<title: Amazon запатентовала следящий за руками работников браслет\n",
      "text: Компания Amazon запатентовала наручный браслет, который будет отслеживать положение рук работников склада и контролировать их действия. Если сотрудник начнет класть товар в неправильный контейнер, так... > \n",
      "\n",
      "<title: SpaceX протестировала посадку первой ступени Falcon 9 на трех двигателях\n",
      "text: SpaceX протестировала режим посадки первой ступени ракеты Falcon 9 с использованием трех двигателей, сообщил Илон Маск в твиттере. Поскольку посадка была экспериментальной, ступень не сажали на платфо... > \n",
      "\n",
      "<title: Американский флот получил робота — охотника за подлодками\n",
      "text: Агентство перспективных оборонных разработок (DARPA) министерства обороны США завершило свою часть проекта ACTUV по созданию полностью автономного надводного робота, предназначенного для обнаружения и... > \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n1=GetNPlus1()\n",
    "n1.loadArticles(\"data/nplus1_test.txt\")\n",
    "# Проверяем как работает __len__()\n",
    "#len(n1)\n",
    "\n",
    "# Проверяем как работают срезы.\n",
    "#len(n1[1:5])\n",
    "#type(n1[1:5])\n",
    "#n1[1].title\n",
    "\n",
    "# Инициализирующий конструктор.\n",
    "#n2=GetNPlus1(n1)\n",
    "#n2.articles[0].title, n1.articles[0].title\n",
    "\n",
    "# Выдача свойств, которых нет у объекта - спорная практическая значимость и очевидность кода.\n",
    "#n1.A, n1.b\n",
    "\n",
    "# Работа с @property\n",
    "#print(n1.morph)\n",
    "#print(n1.__morph)\n",
    "\n",
    "# Операторы сдвига и сложения для разных типов.\n",
    "#n1<<n1[0]\n",
    "#n1+=n1[0]\n",
    "#n1<<1\n",
    "#print(\"\", n1.A, \"\\n\", n1[-1].title)\n",
    "\n",
    "# \"Левое\" сложение против \"правого\".\n",
    "#n2=n1+n1[0]\n",
    "#n2=n1[0]+n1\n",
    "#print(\"\", n2.A, \"\\n\", n2[-1].title)\n",
    "\n",
    "# Преобразование к строке.\n",
    "#print(n1)\n",
    "\n",
    "# \"Вызов\" функции как объекта.\n",
    "#n1()\n",
    "\n",
    "# Тестируем __str__()\n",
    "#print(n1.getArticle(\"https://nplus1.ru/news/2019/02/20/deep-sqeak\"))\n",
    "# Тестируем __repr__()\n",
    "#n1[0]\n",
    "\n",
    "# Тестируем коллекцию статей как итерируемый объект.\n",
    "for art in n1[2:10]:\n",
    "    print(art, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большим удивлением для меня было узнать, что свойства с двумя подчеркиваниями перестали быть видимыми в новых версиях Python. Пользуйтесь свойствами!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Test1' object has no attribute '__x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a2ecaa588606>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Test1' object has no attribute '__x'"
     ]
    }
   ],
   "source": [
    "class Test1:\n",
    "    def __init__(self):\n",
    "        self.__x=0\n",
    "        self.__y=0\n",
    "    \n",
    "    def printX(self):\n",
    "        print(self.__x)\n",
    "        \n",
    "tst=Test1()\n",
    "tst.printX()\n",
    "print(\"===\")\n",
    "print(tst.__x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но при помощи [библиотек](https://habr.com/ru/post/443192/) можно добиться еще большего."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
